<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Qi Bi</title>

    <meta name="author" content="Qi Bi">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

   <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Qi Bi
                </p>
                <p>He is currently a Postdoc Research Fellow at the <a href="https://www.uu.nl/en/organisation/department-of-information-and-computing-sciences">Department of Information and Computing Sciences</a> of <a href="https://www.uu.nl/en">Utrecht university</a>, 
                  advised by <a href="https://webspace.science.uu.nl/~salah006/"> Prof. Albert Ali Salah</a> and <a href="https://www.uu.nl/medewerkers/RCVeltkamp"> Prof. Remco Veltkamp</a>.
                From September 2024 to May 2025, He worked as a lecturer jointly at the faculty of science and the Informatics Institute of <a href="https://www.uva.nl/">University of Amsterdam</a>.</p>
                 <p> He obtained his doctorate degree at the <a href="https://ivi.fnwi.uva.nl/cv/">Computer Vision Research Group</a> in <a href="https://www.uva.nl/">University of Amsterdam</a>, promoted by 
  <a href="https://staff.fnwi.uva.nl/th.gevers/"> Prof. Theo Gevers</a> and <a href="https://youshaodi.github.io/"> Dr. Shaodi You</a>.
                Previous to that, He obtained his bachelor (BEng) and master degree (MSc) from <a href="https://en.whu.edu.cn/"> Wuhan University</a>
  in 2017 and 2020, advised by Prof. Kun Qin and <a href="http://www.captain-whu.com/xia_En.html"> Prof. Gui-song Xia</a>.
In early 2020, He did research internship at the 
  <a href="https://jarvislab.tencent.com/index.html"> Jarvis Research Center, Tencent Youtu Lab</a>, led by 
  <a href="https://sites.google.com/site/yefengzheng"> Dr. Yefeng Zheng</a>.
<p> His current research interests include learning inductive bias to handle the distribution shift from various domains & modalities, and multi-modality perception & understanding, with a particular focus on the application of long video/text understanding & generation.
  In the past five years, his research interests have focused on learning a generalizable representation for visual intelligence, covering multiple application domains such as autonomous driving, remote sensing and medical imaging.
</p>
                 
</p> Email: q.bi[at]ieee.org; q.bi[at]uu[dot]nl; q_bi[at]whu[dot]edu[dot]cn.
</p>
              
                <p style="text-align:center">
                  <a href="mailto:q.bi@ieee.org">Email</a> &nbsp;/&nbsp;
                  <a href="data/Qi Bi-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=v6RAqYwAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/BiQiWHU">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/qi-bi-574604195/">Linkedin</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Qi Bi.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/Qi Bi.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Honors and Awards</h2>
                <p> Awarded as an <a href="https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee">outstanding reviewer</a> for CVPR2025 (top 5.6%, 711/12593). </p>
                <p> Awarded as a <a href="https://neurips.cc/Conferences/2024/ProgramCommittee">top reviewer</a> for NeurIPS2024. </p>
                <p> Awarded as an <a href="https://bmvc2024.org/people/reviewers/">outstanding reviewer</a> for BMVC2024 (166/840). </p>
                <p> Awarded as an <a href="https://link.springer.com/journal/11263/updates/27620812">outstanding reviewer</a>  for International Journal of Computer Vision (IJCV) in the year 2023. </p>
                <p> Awarded as an <a href="https://cvpr2023.thecvf.com/Conferences/2023/OutstandingReviewers">outstanding reviewer</a> for CVPR2023 (top 3.3%, 232/7403). </p>
<p> Work <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Learning_Calibrated_Medical_Image_Segmentation_via_Multi-Rater_Agreement_Modeling_CVPR_2021_paper.pdf"> multi-rater medical image segmentation</a> was shortlisted for 
  <a href="https://cvpr2021.thecvf.com/node/290"> CVPR2021 best paper candidate</a> (top 0.46%, 32/7015). 
   </p> 
<p> Work <a href="https://link.springer.com/chapter/10.1007/978-3-030-87237-3_6"> multi-instance medical image diagnosis</a> was shortlisted for <a href="https://www.miccai2021.org/en/MICCAI-2021-TRAVEL-AWARDS.html"> MICCAI2021 travel awards</a>. </p> 
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

</tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Professional Activities</h2>              
<p> 
  Area Chair for 
  <a href="https://acmmm2025.org/"> ACM MM</a> (2025), <a href="https://wacv.thecvf.com/"> WACV</a> (2026).
</p>
<p> 
  Regular reviewer for 
  <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34"> TPAMI</a>,
  <a href="https://www.springer.com/journal/11263"> IJCV</a>, 
  <a href="https://ieeexplore.ieee.org/xpl/aboutJournal.jsp?punumber=83"> TIP</a> (2021-present).
</p>
<p> 
  Regular reviewer for
  <a href="https://cvpr2022.thecvf.com/"> CVPR</a>,
  <a href="https://iccv2023.thecvf.com/"> ICCV</a>,
  <a href="https://eccv2022.ecva.net/"> ECCV</a> (2021-present). 
</p>
<p> 
  Regular reviewer for
  <a href="https://nips.cc/virtual/2022/index.html"> NeurIPS</a>,
  <a href="https://icml.cc/Conferences/2024"> ICML</a>,
  <a href="https://iclr.cc/"> ICLR</a> (2022-present).
</p>
<p> 
  Regular reviewer for
  <a href="https://aaai.org/aaai-conference/"> AAAI</a>,
  <a href="https://ijcai24.org/"> IJCAI</a>,
  <a href="https://2024.acmmm.org/"> ACM MM</a> (2022-present). 
</p>

<p> IEEE Member with <a href="https://signalprocessingsociety.org/"> Signal Processing Society</a>. </p>
                
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          
</tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                His research focuses on learning a generalizable representation for visual intelligence, covering multiple application domains such as autonomous driving, medical imaging and aerial imaging. Some representative publications are listed below.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/CVPR2025NightAdapter.png' alt="" width="160" height="130">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href=" ">
                  <span class="papertitle">NightAdapter: Learning a Frequency Adapter for Generalizable Night-time Scene Segmentation</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Jingjun Yi, Huimin Huang, Hao Zheng, Haolan Zhan, Yawen Huang, Yuexiang Li, Xian Wu, Yefeng Zheng
                <br>
                <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025
                <br>
                <br>
                <a href="https://github.com/BiQiWHU/NightAdapter">code</a>
                <br>
                <p></p>
                <p>Advancing vision foundation model to domain generalized night-time scene segmentation. The side adapter leverages the discrete sine prior to gain more illumination robustness.</p>
              </td>
            </tr>
        
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/AAAI2025HSSH.png' alt="" width="160" height="130">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32180">
                  <span class="papertitle">Learning Fine-grained Domain Generalization via Hyperbolic State Space Hallucination</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Jingjun Yi, Haolan Zhan, Wei Ji, Gui-Song Xia
                <br>
                <em>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</em>, 2025
                <br>
                <br>
                <a href="https://github.com/BiQiWHU/HSSH">code</a>
                <br>
                <p></p>
                <p>Advancing selective state space model to fine-grained domain generalization by style hallucination in the hyperbolic manifold.</p>
              </td>
            </tr>
            
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/AAAI2025DGFamba.png' alt="" width="160" height="130">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32181">
                  <span class="papertitle">DGFamba: Learning Flow Factorized State Space for Visual Domain Generalization</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Jingjun Yi, Hao Zheng, Haolan Zhan, Wei Ji, Yawen Huang, Yuexiang Li
                <br>
                <em>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</em>, 2025
                <br>
                <br>
                <p></p>
                <p>Learning domain generalized selective state space by flow factorization.</p>
              </td>
            </tr>
            
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/TIP2025CGL.png' alt="" width="160" height="130">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/10829548">
                  <span class="papertitle">
Universal Fine-Grained Visual Categorization by Concept Guided Learning</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Beichen Zhou, Wei Ji, Gui-Song Xia
                <br>
                <em>IEEE Transactions on Image Processing (T-IP)</em>, 2025
                <br>
                <br>
                 <a href="https://github.com/BiQiWHU/CGL">code</a>
                <br>
                <br>
                 <a href="https://drive.google.com/file/d/11hYbdO32hyspucDKp5wwjwvCaD38AEKe/view">dataset</a>
                 <br>
                <p></p>
                <p>Advancing fine-grained visual categorization (FGVC) from object-centric to scene-centric and adverse viewpoints; proposing a fine-grained land-cover dataset (FGLCD); feasible to multiple fine-grained categorization and detection tasks.</p>
              </td>
            </tr>
            
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/NeurIPS2024Samba.png' alt="" width="160" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/8aa0c4d28021c0b273480f9e2aab83a6-Paper-Conference.pdf">
                  <span class="papertitle">Samba: Severity-aware Recurrent Modeling for Cross-domain Medical Image Grading</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Jingjun Yi, Hao Zheng, Wei Ji, Haolan Zhan, Yawen Huang, Yuexiang Li, Yefeng Zheng
                <br>
                <em>Annual Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024
                <br>
                <br>
                 <a href="https://github.com/BiQiWHU/Samba">code</a>
                <p></p>
                <p>A Severity-aware Recurrent Modeling method, dubbed as Samba, is proposed for general disease grading within- and cross-domain medical images on three modalities.</p>
              </td>
            </tr>

<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/NeurIPS2024FADA.png' alt="" width="160" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/aaf50c91c3fc018f6a476032d02114d9-Paper-Conference.pdf">
                  <span class="papertitle">Learning Frequency-Adapted Vision Foundation Model for Domain Generalized Semantic Segmentation</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Jingjun Yi, Hao Zheng, Haolan Zhan, Yawen Huang, Wei Ji, Yuexiang Li, Yefeng Zheng
                <br>
                <em>Annual Conference on Neural Information Processing Systems (NeurIPS)</em>, 2024
                <br>
                <br>
                 <a href="https://github.com/BiQiWHU/FADA">code</a>
                <p></p>
                <p>A frequency-aware parameter-efficient fine-tuning method, dubbed as FADA, is proposed for domain generalized semantic segmentation. It is feasible to various vision foundation model.</p>
              </td>
            </tr>    

   
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/AAAI2024CMFormer.png' alt="" width="160" height="130">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27840/27706">
                  <span class="papertitle">Learning Content-enhanced Mask Transformer for Domain Generalized Urban-scene Segmentation</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Shaodi You, Theo Gevers
                <br>
                <em>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</em>, 2024
                <br>
                <br>
                 <a href="https://github.com/BiQiWHU/CMFormer">code</a>
                <p></p>
                <p>Learning domain generalized scene segmentation by content-enhanced mask attention mechanism.</p>
              </td>
            </tr>
            
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/AAAI2024fog2.png' alt="" width="160" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27838/27702">
                  <span class="papertitle">Learning Generalized Segmentation for Foggy-Scenes by Bi-directional Wavelet Guidance</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Shaodi You, Theo Gevers
                <br>
                <em>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</em>, 2024
                <br>
                <br>
                 <a href="https://github.com/BiQiWHU/BWG">code</a>
                <p></p>
                <p>Learning scene segmentation that can be generalized to arbitrary unseen foggy target domains from only a clear source domain; the first work for this task.</p>
              </td>
            </tr>

<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/AAAI2024DFQ.png' alt="" width="160" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27839/27704">
                  <span class="papertitle">Learning Generalized Medical Image Segmentation from Decoupled Feature Queries</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Jingjun Yi, Hao Zheng, Wei Ji, Yawen Huang, Yuexiang Li, Yefeng Zheng
                <br>
                <em>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</em>, 2024
                <br>
                <br>
                 <a href="https://github.com/BiQiWHU/DFQ">code</a>
                <p></p>
                <p>Learning domain generalized medical image segmentation by querying from decoupled features; the first work to leverage Vision Transformer for domain generalized medical image segmentation.</p>
              </td>
            </tr>            
            
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/TIP2023AD.png' alt="" width="160" height="130">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/10176286">
                  <span class="papertitle">
Interactive Learning of Intrinsic and Extrinsic Properties for All-day Semantic Segmentation</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Shaodi You, Theo Gevers
                <br>
                <em>IEEE Transactions on Image Processing (T-IP)</em>, 2023
                <br>
                <br>
                 <a href="https://github.com/BiQiWHU/All-day-CityScapes-segmentation">code</a>
                <br>
                <br>
                 <a href="https://isis-data.science.uva.nl/cv/1ADcityscape.zip">dataset</a>
                 <br>
                <p></p>
                <p>Learning robust scene semantic segmentation under all-day scenarios; proposing the first all-day semantic segmentation dataset All-day CityScapes.</p>
              </td>
            </tr>
        
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/CVPRW2023SAM.png' alt="" width="160" height="130">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/pdf/2304.05750">
                  <span class="papertitle">Segment anything is not always perfect: An investigation of sam on different real-world applications</span>
                </a>
                <br>
                Wei Ji, Jingjing Li, <strong>Qi Bi</strong>, Wenbo Li, Li Cheng
                <br>
                <em>CVPR 1st workshop on Vision-based InduStrial InspectiON</em>, 2023
                <br>
                <br>
               <font color="red"><strong>Best paper award</strong></font>
                <br>
                <br>
                 <a href="https://github.com/LiuTingWed/SAM-Not-Perfect">code</a>
                <p></p>
                <p>Benchmarking Segment Anything (SAM) on multiple real-world scenarios.</p>
              </td>
            </tr>

<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/ICLR2022USOD.png' alt="" width="160" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://openreview.net/pdf?id=BZnnMbt0pW">
                  <span class="papertitle">Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection</span>
                </a>
                <br>
                Wei Ji, Jingjing Li, <strong>Qi Bi</strong>, Chuan Guo, Jie Liu, Li Cheng
                <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2022
                <br>
                <p></p>
                 <a href="https://github.com/jiwei0921/DSU">code</a>
                <p></p>
                <p>Learning deep unsupervised RGB-D saliency detection, by engaging depth information to improve pseudo-labels in the training process.</p>
              </td>
            </tr>

       
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/TGRS2022AGOS.png' alt="" width="160" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9866803">
                  <span class="papertitle">All Grains, One Scheme (AGOS): Learning Multi-grain Instance Representation for Aerial Scene Classification</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Beichen Zhou, Kun Qin, Qinghao Ye, Gui-Song Xia
                <br>
                <em>IEEE Transactions on Geoscience and Remote Sensing (T-GRS)</em>, 2022
                <br>
                <br>
                 <a href="https://github.com/BiQiWHU/AGOS">code</a>
                <p></p>
                <p>Extending deep multiple instance learning into a multi-grain framework while maintaining the same semantic scheme, dubbed as AGOS; learning discriminative aerial scene representation by AGOS.</p>
              </td>
            </tr>
    
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/AAAI2022Semi.png' alt="" width="160" height="130">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ojs.aaai.org/index.php/AAAI/article/download/20098/19857">
                  <span class="papertitle">Label-efficient Hybrid-supervised Learning for Medical Image Segmentation</span>
                </a>
                <br>
                Junwen Pan*, <strong>Qi Bi*</strong>, Yanzhan Yang, Pengfei Zhu, Cheng Bian 
                <br>
                <p></p>
                <br>
                * : equal contribution
                <br>
                <em>Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</em>, 2022
                <br>
                <p></p>
                <p>Learning weakly semi-supervised medical image segmentation by the proposed dynamic instance indicator and dynamic co-regularization framework.</p>
              </td>
            </tr>

        
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/TIP2021LSE.png' alt="" width="160" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9477300">
                  <span class="papertitle">Local semantic enhanced convnet for aerial scene recognition</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Kun Qin, Han Zhang, Gui-Song Xia
                <br>
                <em>IEEE Transactions on Image Processing (T-IP)</em>, 2021
                <br>
                <br>
                 <a href="https://github.com/BiQiWHU/LSENet">code</a>
                <p></p>
                <p>Learning aerial scene representation by modeling context-aware class peak response.</p>
              </td>
            </tr>

<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/NIPS2021SOD.png' alt="" width="160" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/642e92efb79421734881b53e1e1b18b6-Paper.pdf">
                  <span class="papertitle">Joint semantic mining for weakly supervised RGB-D salient object detection</span>
                </a>
                <br>
                Jingjing Li, Wei Ji, <strong>Qi Bi</strong>, Cheng Yan, Miao Zhang, Yongri Piao, Huchuan Lu
                <br>
                <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2021
                <br>
                <p></p>
                 <a href="https://github.com/jiwei0921/JSM">code</a>
                <p></p>
                 <a href="https://drive.google.com/file/d/1Oy9OGvQD2H7xrV9WH1j3n8xNS2UVT1cY/view?usp=sharing">dataset</a>
                <p></p>
                <p>Learning weakly-supervised RGB-D salient object detection (SOD) from the image, depth map and image caption; proposing a dataset for caption based SOD dubbed as CapS.</p>
              </td>
            </tr>

<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/MICCAI2021MIL.png' alt="" width="160" height="160">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://link.springer.com/chapter/10.1007/978-3-030-87237-3_6">
                  <span class="papertitle">Local-global dual perception based deep multiple instance learning for retinal disease classification</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Shuang Yu, Wei Ji, Cheng Bian, Lijun Gong, Hanruo Liu, Kai Ma, Yefeng Zheng
                <br>
                <em>Medical Image Computing and Computer Assisted Intervention (MICCAI)</em>, 2021
                <br>
                 <br>
               <font color="red"><strong>MICCAI2021 travel awards</strong></font>
                <br>
                <br>
               <font color="red"><strong>MICCAI2021 young scientist award candidate</strong></font>
                <br>
                <p></p>
                <p>Learning retinal diseases from fundus images by local-global representation.</p>
              </td>
            </tr>
            
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/CVPR2021MultiRater.png' alt="" width="160" height="160">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Learning_Calibrated_Medical_Image_Segmentation_via_Multi-Rater_Agreement_Modeling_CVPR_2021_paper.pdf">
                  <span class="papertitle">Learning calibrated medical image segmentation via multi-rater agreement modeling</span>
                </a>
                <br>
                Wei Ji, Shuang Yu, Junde Wu, Kai Ma, Cheng Bian, <strong>Qi Bi</strong>, Jingjing Li, Hanruo Liu, Li Cheng, Yefeng Zheng
                <br>
                <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
                <br>
                <br>
               <font color="red"><strong>Best paper candidate</strong></font>
                <br>
                <br>
                 <a href="https://github.com/jiwei0921/MRNet/">code</a>
                <p></p>
                <p>Learning medical image segmentation from multiple annotations by multi-rater modeling.</p>
              </td>
            </tr>

            
<tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/TIP2020MIDC.png' alt="" width="160" height="160">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9023551">
                  <span class="papertitle">A multiple-instance densely-connected ConvNet for aerial scene classification</span>
                </a>
                <br>
                <strong>Qi Bi</strong>, Kun Qin, Zhili Li, Han Zhang, Kai Xu, Gui-Song Xia
                <br>
                <em>IEEE Transactions on Image Processing (T-IP)</em>, 2020
                <br>
                <br>
                 <a href="https://github.com/BiQiWHU/Attention-based-Multi-instance-CNN">code</a>
                <p></p>
                <p>Modeling discriminative aerial scene representation by deep multiple instance learning.</p>
              </td>
            </tr>
            
</tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Supervision</h2>              
<p> Noud Corten, Improved Road Crack Severity Measurement Using Deep Convolutional Networks by Storing Spatial Information, November 2021-August 2022 (completed). </p>
<p> Carlo Airaghi, Multi-Stage Multiscale Training Architecture for Semantic Segmentation of Remote Sensing Images, April 2021- December 2021 (completed). </p>
<p> Silvan Murre, Layout2Land: Semi-Supervised Learning of a Layout and Style Reconfigurable GAN, March 2021-June 2021 (completed). </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
</tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Teaching</h2>   
<p> 2025 Computer Vision 2 (UvA, Lecturer) </p>
<p> 2024 Vision & Autonomous Robotics (UvA, Lecturer) </p>
<p> 2024 Computer Vision 1 (UvA, Lecturer) </p>
<p> 2024 Computer Vision 2 (UvA, Teaching Assistant) </p>
<p> 2023 Computer Vision 1 (UvA, Teaching Assistant) </p>
<p> 2023 Computer Vision 2 (UvA, Teaching Assistant) </p>
<p> 2022 Computer Vision 1 (UvA, Teaching Assistant) </p>
<p> 2021 Computer Vision 1 (UvA, Teaching Assistant) </p>
<p> 2020 Computer Vision 1 (UvA, Teaching Assistant) </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

</tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website is based on Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
